{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d4eb64dc",
      "metadata": {
        "id": "d4eb64dc"
      },
      "source": [
        "SETTING UP THE ENVIRONMENT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.dropbox.com/s/fxn3ldztzwxm0rw/FruitsData.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xn5QUFEc7UbX",
        "outputId": "f5e69ace-5a68-4222-b9bd-66ed2dd94d09"
      },
      "id": "Xn5QUFEc7UbX",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-24 06:36:21--  https://www.dropbox.com/s/fxn3ldztzwxm0rw/FruitsData.zip\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.8.18, 2620:100:601b:18::a27d:812\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.8.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/fxn3ldztzwxm0rw/FruitsData.zip [following]\n",
            "--2023-09-24 06:36:21--  https://www.dropbox.com/s/raw/fxn3ldztzwxm0rw/FruitsData.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucac5dc2cc56af18c838e798abfd.dl.dropboxusercontent.com/cd/0/inline/CEV0hBtX4StrD0qCxJFJLQiYd5h0EC0nGlj-Wlv8ka63Uz3qbiTSiXe2Wn7dK9CU-LFpVb3BfmWLdrr4i0hKkzEM2hNyj-PYw-NsMdaLcMZK06HhdXkREfqNwaJNjquifcX-XsY8P2HrHS_qg50Q2ww6/file# [following]\n",
            "--2023-09-24 06:36:21--  https://ucac5dc2cc56af18c838e798abfd.dl.dropboxusercontent.com/cd/0/inline/CEV0hBtX4StrD0qCxJFJLQiYd5h0EC0nGlj-Wlv8ka63Uz3qbiTSiXe2Wn7dK9CU-LFpVb3BfmWLdrr4i0hKkzEM2hNyj-PYw-NsMdaLcMZK06HhdXkREfqNwaJNjquifcX-XsY8P2HrHS_qg50Q2ww6/file\n",
            "Resolving ucac5dc2cc56af18c838e798abfd.dl.dropboxusercontent.com (ucac5dc2cc56af18c838e798abfd.dl.dropboxusercontent.com)... 162.125.8.15, 2620:100:601b:15::a27d:80f\n",
            "Connecting to ucac5dc2cc56af18c838e798abfd.dl.dropboxusercontent.com (ucac5dc2cc56af18c838e798abfd.dl.dropboxusercontent.com)|162.125.8.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CEUb4EuBBIZwCFguzmYutsDkbBOaGLjPRYNf_vouysapOp18rMf2fUrwaiWpLlWCfExVkCJty9mTSRqOmbgKFG2OVzUirDtQC_JdwentQAJCjYGbnKPsRgipdnmU4V1sBLRMuFiO6xte1O95j4yWu9NWw9bZerjhEDXnbdx3jXXzSTfr_ZMrp_38Eyduuuk6vSFNRMqv650nh938x1LyfVQBZwhOSfsxYjVAysSWFb_dStENNL4qwXTQa6bzHSe-RS5KqiD5ErHxBTKppOxbocxNHBL7x96wWT4HkuXgVRcYL42YKAJ0FcBXbV7rQOE4SsvE0PSn4ER_KqWWvXfEgsQdNzQOGiYb2-cln1zoZsPZagWykkvlilW8vvOKODyLsfA/file [following]\n",
            "--2023-09-24 06:36:22--  https://ucac5dc2cc56af18c838e798abfd.dl.dropboxusercontent.com/cd/0/inline2/CEUb4EuBBIZwCFguzmYutsDkbBOaGLjPRYNf_vouysapOp18rMf2fUrwaiWpLlWCfExVkCJty9mTSRqOmbgKFG2OVzUirDtQC_JdwentQAJCjYGbnKPsRgipdnmU4V1sBLRMuFiO6xte1O95j4yWu9NWw9bZerjhEDXnbdx3jXXzSTfr_ZMrp_38Eyduuuk6vSFNRMqv650nh938x1LyfVQBZwhOSfsxYjVAysSWFb_dStENNL4qwXTQa6bzHSe-RS5KqiD5ErHxBTKppOxbocxNHBL7x96wWT4HkuXgVRcYL42YKAJ0FcBXbV7rQOE4SsvE0PSn4ER_KqWWvXfEgsQdNzQOGiYb2-cln1zoZsPZagWykkvlilW8vvOKODyLsfA/file\n",
            "Reusing existing connection to ucac5dc2cc56af18c838e798abfd.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 796742678 (760M) [application/zip]\n",
            "Saving to: ‘FruitsData.zip’\n",
            "\n",
            "FruitsData.zip      100%[===================>] 759.83M  49.4MB/s    in 10s     \n",
            "\n",
            "2023-09-24 06:36:33 (73.6 MB/s) - ‘FruitsData.zip’ saved [796742678/796742678]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q \"/content/FruitsData.zip\""
      ],
      "metadata": {
        "id": "Xr-aw7_Y8X0b"
      },
      "id": "Xr-aw7_Y8X0b",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "783f57c8",
      "metadata": {
        "id": "783f57c8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "tf.random.set_seed(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5fa2250",
      "metadata": {
        "id": "a5fa2250"
      },
      "source": [
        "CREATING THE PATH OBJECTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "bb9320bc",
      "metadata": {
        "id": "bb9320bc"
      },
      "outputs": [],
      "source": [
        "train_path = Path(\"fruits-360/Training\")\n",
        "test_path = Path(\"fruits-360/Test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e0ed981c",
      "metadata": {
        "id": "e0ed981c"
      },
      "outputs": [],
      "source": [
        "train_image_paths = list(train_path.glob(\"*/*\"))\n",
        "train_image_paths = list(map(lambda x: str(x), train_image_paths))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8f26393",
      "metadata": {
        "id": "b8f26393"
      },
      "source": [
        "FETCHING LABELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "95d85129",
      "metadata": {
        "id": "95d85129"
      },
      "outputs": [],
      "source": [
        "def get_labels(image_path):\n",
        "    return image_path.split(\"/\")[-2]\n",
        "\n",
        "train_image_labels = list(map(lambda x : get_labels(x), train_image_paths))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b9e6565",
      "metadata": {
        "id": "4b9e6565"
      },
      "source": [
        "ENCODING THE LABELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "150fe5a0",
      "metadata": {
        "id": "150fe5a0"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "train_image_labels = le.fit_transform(train_image_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd23e7c4",
      "metadata": {
        "id": "cd23e7c4"
      },
      "source": [
        "CREATING ONE-HOT ENCODING FOR FETCHED LABELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "020c1516",
      "metadata": {
        "id": "020c1516"
      },
      "outputs": [],
      "source": [
        "train_image_labels = tf.keras.utils.to_categorical(train_image_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "205bef76",
      "metadata": {
        "id": "205bef76"
      },
      "source": [
        "TRAIN VALIDATION SPLIT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a5b12920",
      "metadata": {
        "id": "a5b12920"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Train_paths, Val_paths, Train_labels, Val_labels = train_test_split(train_image_paths,\n",
        "                                                                   train_image_labels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f43354d9",
      "metadata": {
        "id": "f43354d9"
      },
      "source": [
        "CREATING A TENSOR OBJECT FOR IMAGES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "efa7deb7",
      "metadata": {
        "id": "efa7deb7"
      },
      "outputs": [],
      "source": [
        "def load(image, label):\n",
        "    image = tf.io.read_file(image)\n",
        "    image = tf.io.decode_jpeg(image, channels = 3)\n",
        "    return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bafbd5ce",
      "metadata": {
        "id": "bafbd5ce"
      },
      "source": [
        "DATA AUGMENTATION AND RESIZING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "0e7b274d",
      "metadata": {
        "id": "0e7b274d"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "resize = tf.keras.Sequential([\n",
        "    tf.keras.layers.experimental.preprocessing.Resizing(IMG_SIZE, IMG_SIZE)\n",
        "])\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "    tf.keras.layers.experimental.preprocessing.RandomZoom(height_factor = (-0.3, -0.2))\n",
        "\n",
        "\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2659cbb9",
      "metadata": {
        "id": "2659cbb9"
      },
      "source": [
        "CREATING TENSORFLOW DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d17ebcfc",
      "metadata": {
        "id": "d17ebcfc"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "def get_dataset(paths, labels, train = True):\n",
        "    image_paths = tf.convert_to_tensor(paths)\n",
        "    labels = tf.convert_to_tensor(labels)\n",
        "\n",
        "    image_dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
        "    label_dataset = tf.data.Dataset.from_tensor_slices(labels)\n",
        "\n",
        "    dataset = tf.data.Dataset.zip((image_dataset, label_dataset))\n",
        "\n",
        "    dataset = dataset.map(lambda image, label: load(image, label))\n",
        "    dataset = dataset.map(lambda image, label: (resize(image), label),\n",
        "                          num_parallel_calls = AUTOTUNE)\n",
        "    dataset = dataset.shuffle(1000)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "\n",
        "    if train:\n",
        "        dataset = dataset.map(lambda image, label: (data_augmentation(image), label),\n",
        "                          num_parallel_calls = AUTOTUNE)\n",
        "    dataset = dataset.repeat()\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "015e4417",
      "metadata": {
        "id": "015e4417"
      },
      "outputs": [],
      "source": [
        "train_dataset = get_dataset(Train_paths, Train_labels)\n",
        "val_dataset = get_dataset(Val_paths,  Val_labels, train = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c966183d",
      "metadata": {
        "id": "c966183d"
      },
      "source": [
        "MODEL BUILDING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c450491d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c450491d",
        "outputId": "3a3309f6-69e0-48d0-b3a9-745c78b55d05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94668760/94668760 [==============================] - 0s 0us/step\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " resnet50v2 (Functional)     (None, 7, 7, 2048)        23564800  \n",
            "                                                                 \n",
            " global_average_pooling2d (  (None, 2048)              0         \n",
            " GlobalAveragePooling2D)                                         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 131)               268419    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23833219 (90.92 MB)\n",
            "Trainable params: 23787779 (90.74 MB)\n",
            "Non-trainable params: 45440 (177.50 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications import ResNet50V2\n",
        "backbone = ResNet50V2(\n",
        "          input_shape = (224,224,3),\n",
        "          include_top = False\n",
        ")\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    backbone,\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Dense(131, activation = \"softmax\")\n",
        "])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f49a4ae1",
      "metadata": {
        "id": "f49a4ae1"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001, beta_1 = 0.9, beta_2 = 0.999),\n",
        "    loss = 'categorical_crossentropy',\n",
        "    metrics = ['accuracy', tf.keras.metrics.Precision(name = 'precision'),\n",
        "               tf.keras.metrics.Recall(name = 'recall')]\n",
        "\n",
        "   )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c29da247",
      "metadata": {
        "id": "c29da247"
      },
      "source": [
        "TRAINING THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "0ed65bff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ed65bff",
        "outputId": "8119b745-5b2d-4f9b-adcd-065af3b6ed12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1586/1586 [==============================] - 668s 394ms/step - loss: 0.4455 - accuracy: 0.8755 - precision: 0.9344 - recall: 0.8409 - val_loss: 3.9551 - val_accuracy: 0.4772 - val_precision: 0.5165 - val_recall: 0.4533\n"
          ]
        }
      ],
      "source": [
        "history1 = model.fit(\n",
        "          train_dataset,\n",
        "          steps_per_epoch=len(Train_paths)//BATCH_SIZE,\n",
        "          epochs = 1,\n",
        "          validation_data = val_dataset,\n",
        "          validation_steps=len(Val_paths)//BATCH_SIZE,\n",
        "          verbose = 1\n",
        "         )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "7d8f6049",
      "metadata": {
        "id": "7d8f6049"
      },
      "outputs": [],
      "source": [
        "model.layers[0].trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "619ce374",
      "metadata": {
        "id": "619ce374"
      },
      "source": [
        "ADDING CALLBACKS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "19ab8fae",
      "metadata": {
        "id": "19ab8fae"
      },
      "outputs": [],
      "source": [
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"best_weights.h5\", verbose = 1, save_best_only = True,\n",
        "                                               save_weights_only = True)\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(patience=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "374649cc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "374649cc",
        "outputId": "c73835ed-8c44-45ef-fd40-540c8086cac6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " resnet50v2 (Functional)     (None, 7, 7, 2048)        23564800  \n",
            "                                                                 \n",
            " global_average_pooling2d (  (None, 2048)              0         \n",
            " GlobalAveragePooling2D)                                         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 131)               268419    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23833219 (90.92 MB)\n",
            "Trainable params: 268419 (1.02 MB)\n",
            "Non-trainable params: 23564800 (89.89 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "5d0863c5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d0863c5",
        "outputId": "9fedb241-89aa-47c3-a752-644ecba6d3d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "1586/1586 [==============================] - ETA: 0s - loss: 0.0906 - accuracy: 0.9715 - precision: 0.9750 - recall: 0.9683\n",
            "Epoch 1: val_loss improved from inf to 7.98989, saving model to best_weights.h5\n",
            "1586/1586 [==============================] - 634s 399ms/step - loss: 0.0906 - accuracy: 0.9715 - precision: 0.9750 - recall: 0.9683 - val_loss: 7.9899 - val_accuracy: 0.2411 - val_precision: 0.2571 - val_recall: 0.2310\n",
            "Epoch 2/6\n",
            "1586/1586 [==============================] - ETA: 0s - loss: 0.0566 - accuracy: 0.9833 - precision: 0.9849 - recall: 0.9815\n",
            "Epoch 2: val_loss improved from 7.98989 to 2.64036, saving model to best_weights.h5\n",
            "1586/1586 [==============================] - 617s 387ms/step - loss: 0.0566 - accuracy: 0.9833 - precision: 0.9849 - recall: 0.9815 - val_loss: 2.6404 - val_accuracy: 0.5515 - val_precision: 0.5799 - val_recall: 0.5328\n",
            "Epoch 3/6\n",
            "1586/1586 [==============================] - ETA: 0s - loss: 0.0480 - accuracy: 0.9847 - precision: 0.9860 - recall: 0.9837\n",
            "Epoch 3: val_loss improved from 2.64036 to 1.65915, saving model to best_weights.h5\n",
            "1586/1586 [==============================] - 649s 410ms/step - loss: 0.0480 - accuracy: 0.9847 - precision: 0.9860 - recall: 0.9837 - val_loss: 1.6592 - val_accuracy: 0.6880 - val_precision: 0.7164 - val_recall: 0.6689\n",
            "Epoch 4/6\n",
            "1586/1586 [==============================] - ETA: 0s - loss: 0.0347 - accuracy: 0.9900 - precision: 0.9908 - recall: 0.9895\n",
            "Epoch 4: val_loss improved from 1.65915 to 1.17566, saving model to best_weights.h5\n",
            "1586/1586 [==============================] - 646s 407ms/step - loss: 0.0347 - accuracy: 0.9900 - precision: 0.9908 - recall: 0.9895 - val_loss: 1.1757 - val_accuracy: 0.7400 - val_precision: 0.7705 - val_recall: 0.7190\n",
            "Epoch 5/6\n",
            "1586/1586 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9907 - precision: 0.9914 - recall: 0.9903\n",
            "Epoch 5: val_loss did not improve from 1.17566\n",
            "1586/1586 [==============================] - 638s 402ms/step - loss: 0.0320 - accuracy: 0.9907 - precision: 0.9914 - recall: 0.9903 - val_loss: 1.5130 - val_accuracy: 0.7026 - val_precision: 0.7247 - val_recall: 0.6859\n",
            "Epoch 6/6\n",
            "1586/1586 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9934 - precision: 0.9937 - recall: 0.9930\n",
            "Epoch 6: val_loss did not improve from 1.17566\n",
            "1586/1586 [==============================] - 638s 402ms/step - loss: 0.0226 - accuracy: 0.9934 - precision: 0.9937 - recall: 0.9930 - val_loss: 5.7400 - val_accuracy: 0.4077 - val_precision: 0.4251 - val_recall: 0.3967\n"
          ]
        }
      ],
      "source": [
        "history2 = model.fit(train_dataset,\n",
        "          steps_per_epoch=len(Train_paths)//BATCH_SIZE,\n",
        "          epochs = 6,\n",
        "          callbacks=[checkpoint, early_stop],\n",
        "          validation_data = val_dataset,\n",
        "          validation_steps=len(Val_paths)//BATCH_SIZE,\n",
        "          verbose = 1\n",
        "         )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TESTING"
      ],
      "metadata": {
        "id": "LHL6hhG02vPw"
      },
      "id": "LHL6hhG02vPw"
    },
    {
      "cell_type": "code",
      "source": [
        "test_image_paths = list(test_path.glob('*/*'))\n",
        "test_image_paths = list(map(lambda x: str(x), test_image_paths))\n",
        "test_labels = list(map(lambda x : get_labels(x), test_image_paths))\n",
        "\n",
        "test_labels = le.fit_transform(test_labels)\n",
        "test_labels = tf.keras.utils.to_categorical(test_labels)\n",
        "\n",
        "\n",
        "test_image_paths = tf.convert_to_tensor(test_image_paths)\n",
        "test_labels = tf.convert_to_tensor(test_labels)\n",
        "\n",
        "def decode_image(image, label):\n",
        "    image = tf.io.read_file(image)\n",
        "    image = tf.io.decode_jpeg(image, channels = 3)\n",
        "    image = tf.image.resize(image, [224,224], method = \"bilinear\")\n",
        "    return image, label\n",
        "\n",
        "test_dataset = (\n",
        "      tf.data.Dataset\n",
        "     .from_tensor_slices((test_image_paths, test_labels))\n",
        "     .map(decode_image)\n",
        "     .batch(BATCH_SIZE)\n",
        ")"
      ],
      "metadata": {
        "id": "Yah-Z9OD21w-"
      },
      "id": "Yah-Z9OD21w-",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "loss, acc, prec, rec = model.evaluate(test_dataset)\n",
        "\n",
        "print(\"loss is: \", loss)\n",
        "print(\"acc is: \", acc)\n",
        "print(\"prec is: \", prec)\n",
        "print(\"rec is: \", rec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7bH9cRs5BHW",
        "outputId": "f72a77aa-6541-456f-d18f-a4f6cc5440a7"
      },
      "id": "I7bH9cRs5BHW",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "709/709 [==============================] - 65s 92ms/step - loss: 6.4739 - accuracy: 0.3724 - precision: 0.3886 - recall: 0.3627\n",
            "loss is:  6.473881244659424\n",
            "acc is:  0.3724435865879059\n",
            "prec is:  0.3885819613933563\n",
            "rec is:  0.36270275712013245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uk0Ozask51UC"
      },
      "id": "Uk0Ozask51UC",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.11.5 (myenv)",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}